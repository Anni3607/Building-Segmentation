# -*- coding: utf-8 -*-
"""Building Segmentation

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kDnQqySrPzt5367TTPukKchUcsmcAYev
"""

# Install required packages
!pip install -q albumentations
!pip install -q tensorflow-addons

# Mount Google Drive (if you store dataset there)
from google.colab import drive
drive.mount('/content/drive')

# If your zip is on Drive, copy it. Adjust the path if needed.
!cp '/content/drive/My Drive/inria_subset_50_files.zip' . || true

# Unzip (if already unzipped this will overwrite/skip)
import zipfile, os
if os.path.exists('inria_subset_50_files.zip'):
    with zipfile.ZipFile('inria_subset_50_files.zip', 'r') as zip_ref:
        zip_ref.extractall("inria_dataset")

# Show some directories
!ls -lah /content/inria_dataset | sed -n '1,200p' || true
!ls -lah /content/inria_dataset/images | sed -n '1,200p' || true
!ls -lah /content/inria_dataset/gt | sed -n '1,200p' || true

# Quick EDA: display one example
import cv2
import matplotlib.pyplot as plt
img_path = '/content/inria_dataset/images/austin1.tif'
mask_path = '/content/inria_dataset/gt/austin1.tif'
if os.path.exists(img_path) and os.path.exists(mask_path):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    msk = cv2.imread(mask_path, 0)
    plt.figure(figsize=(10,5))
    plt.subplot(1,2,1); plt.title("Image"); plt.imshow(img); plt.axis('off')
    plt.subplot(1,2,2); plt.title("Mask"); plt.imshow(msk, cmap='gray'); plt.axis('off')
    plt.show()
else:
    print("Example files not found at expected path. Check dataset location.")

import os, glob, cv2, numpy as np
from tqdm import tqdm

IMG_DIR = "/content/inria_dataset/images"
MSK_DIR = "/content/inria_dataset/gt"

PATCH_DIR_IMG = "/content/inria_patches/images"
PATCH_DIR_MSK = "/content/inria_patches/masks"
os.makedirs(PATCH_DIR_IMG, exist_ok=True)
os.makedirs(PATCH_DIR_MSK, exist_ok=True)

PATCH = 256       # patch size
STRIDE = 128      # overlapping stride
MAX_BASE_IMAGES = None   # set to None to use all base images, or an int to limit

image_paths = sorted(glob.glob(os.path.join(IMG_DIR, "*.tif")))
mask_paths  = sorted(glob.glob(os.path.join(MSK_DIR, "*.tif")))
print("Found images:", len(image_paths), "masks:", len(mask_paths))

def tile_one_pair(img_path, msk_path, base_name, patch_size=PATCH, stride=STRIDE):
    img = cv2.imread(img_path)                # BGR
    msk = cv2.imread(msk_path, 0)             # gray
    h, w = msk.shape[:2]
    count = 0
    for y in range(0, h - patch_size + 1, stride):
        for x in range(0, w - patch_size + 1, stride):
            img_patch = img[y:y+patch_size, x:x+patch_size]
            msk_patch = msk[y:y+patch_size, x:x+patch_size]

            # Skip completely empty mask tiles to save space/training time
            if msk_patch.sum() == 0:
                continue

            ip = os.path.join(PATCH_DIR_IMG, f"{base_name}_{count}.png")
            mp = os.path.join(PATCH_DIR_MSK, f"{base_name}_{count}.png")
            cv2.imwrite(ip, img_patch)
            cv2.imwrite(mp, msk_patch)
            count += 1
    return count

total = 0
pairs = list(zip(image_paths, mask_paths))
for i, (ip, mp) in enumerate(tqdm(pairs)):
    if MAX_BASE_IMAGES is not None and i >= MAX_BASE_IMAGES:
        break
    bn = os.path.splitext(os.path.basename(ip))[0]
    made = tile_one_pair(ip, mp, bn)
    total += made
    print(f"Tiled {bn}: {made} patches")

print("Total patches saved:", total)

# Quick check
print("Sample patch lists:")
print(len(glob.glob(os.path.join(PATCH_DIR_IMG, "*.png"))), "image patches")
print(len(glob.glob(os.path.join(PATCH_DIR_MSK, "*.png"))), "mask patches")

# Optional: convert PNG patches to JPG with quality 85 to save drive space
IMG_JPG_DIR = "/content/inria_patches_jpg/images"
os.makedirs(IMG_JPG_DIR, exist_ok=True)
for p in glob.glob(os.path.join(PATCH_DIR_IMG, "*.png")):
    im = cv2.imread(p)
    out = os.path.join(IMG_JPG_DIR, os.path.basename(p).replace(".png", ".jpg"))
    cv2.imwrite(out, im, [int(cv2.IMWRITE_JPEG_QUALITY), 85])
print("Converted patches to JPG at", IMG_JPG_DIR)

import albumentations as A
from sklearn.model_selection import train_test_split
import numpy as np
import random
import tensorflow as tf

# Gather lists
img_files = sorted(glob.glob(os.path.join(PATCH_DIR_IMG, "*.png")))
msk_files = sorted(glob.glob(os.path.join(PATCH_DIR_MSK, "*.png")))
assert len(img_files) == len(msk_files), "Image/mask counts differ!"

# Train/Val split
X_train, X_val, y_train, y_val = train_test_split(
    img_files, msk_files, test_size=0.20, random_state=42
)
print("Train:", len(X_train), "Val:", len(X_val))

# Image params
IMG_SIZE = 256
BATCH = 4   # Lower if GPU memory is limited

# Augmentations
train_transform = A.Compose([
    A.RandomRotate90(p=0.5),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.15, rotate_limit=15, p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    A.GaussNoise(p=0.1),
], additional_targets={'mask': 'mask'})

val_transform = A.Compose([], additional_targets={'mask': 'mask'})

# Loader
def load_image_mask_pair(img_path, msk_path, target=IMG_SIZE):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    msk = cv2.imread(msk_path, 0)
    return img, msk

def preprocess(img, msk, target=IMG_SIZE):
    img = cv2.resize(img, (target, target), interpolation=cv2.INTER_LINEAR)
    msk = cv2.resize(msk, (target, target), interpolation=cv2.INTER_NEAREST)
    img = img.astype('float32') / 255.0
    msk = (msk > 127).astype('float32')
    msk = np.expand_dims(msk, axis=-1)
    return img, msk

def batch_generator(paths_x, paths_y, batch_size=BATCH, is_train=True):
    n = len(paths_x)
    idx = np.arange(n)
    transform = train_transform if is_train else val_transform
    while True:
        if is_train:
            np.random.shuffle(idx)
        for start in range(0, n, batch_size):
            end = min(start + batch_size, n)
            batch_idx = idx[start:end]
            imgs, msks = [], []
            for i in batch_idx:
                img, msk = load_image_mask_pair(paths_x[i], paths_y[i])
                augmented = transform(image=img, mask=msk)
                img_a, msk_a = augmented['image'], augmented['mask']
                img_p, msk_p = preprocess(img_a, msk_a)
                imgs.append(img_p)
                msks.append(msk_p)
            yield np.stack(imgs, 0), np.stack(msks, 0)

# Generators
train_gen = batch_generator(X_train, y_train, batch_size=BATCH, is_train=True)
val_gen   = batch_generator(X_val, y_val, batch_size=BATCH, is_train=False)

steps_per_epoch  = max(1, len(X_train) // BATCH)
validation_steps = max(1, len(X_val) // BATCH)

print("Steps per epoch:", steps_per_epoch, "Validation steps:", validation_steps)

from tensorflow.keras import layers, Model
from tensorflow.keras.applications import EfficientNetB0

def upsample_concat(x, skip, filters):
    x = layers.UpSampling2D()(x)
    x = layers.Concatenate()([x, skip])
    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
    x = layers.Conv2D(filters, 3, padding='same', activation='relu')(x)
    return x

def build_unet_effb0(input_shape=(IMG_SIZE, IMG_SIZE, 3), pretrained=True):
    base = EfficientNetB0(include_top=False, input_shape=input_shape, weights='imagenet' if pretrained else None)

    # encoder feature maps to use for skip connections
    # Adjusted layers to ensure compatible spatial dimensions after upsampling
    skips = [
        base.get_layer(name).output for name in
        ["block2a_expand_activation", "block3a_expand_activation", "block4a_expand_activation", "block6a_expand_activation"]
    ]
    x = base.output  # bottleneck

    # decoder path (reverse of skips)
    x = upsample_concat(x, skips[-1], 256)
    x = upsample_concat(x, skips[-2], 128)
    x = upsample_concat(x, skips[-3], 64)
    x = upsample_concat(x, skips[-4], 32)

    # final convs
    x = layers.UpSampling2D()(x)
    x = layers.Conv2D(16, 3, padding='same', activation='relu')(x)
    x = layers.Conv2D(16, 3, padding='same', activation='relu')(x)

    outputs = layers.Conv2D(1, 1, activation='sigmoid')(x)

    model = Model(inputs=base.input, outputs=outputs)
    return model

model = build_unet_effb0((IMG_SIZE, IMG_SIZE, 3), pretrained=True)
model.summary()

import tensorflow.keras.backend as K
import tensorflow as tf

def dice_coef(y_true, y_pred, smooth=1e-7):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)

def dice_loss(y_true, y_pred):
    return 1 - dice_coef(y_true, y_pred)

def bce_dice_loss(y_true, y_pred):
    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)
    return bce + dice_loss(y_true, y_pred)

def iou_metric(y_true, y_pred, smooth=1e-7):
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    y_pred_f = K.round(y_pred_f)
    intersection = K.sum(y_true_f * y_pred_f)
    union = K.sum(y_true_f) + K.sum(y_pred_f) - intersection
    return (intersection + smooth) / (union + smooth)

# Compile model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
              loss=bce_dice_loss,
              metrics=['accuracy', iou_metric, dice_coef])

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

checkpoint = ModelCheckpoint("best_inria_unet.h5", monitor="val_loss", verbose=1, save_best_only=True, mode="min")
reduce_lr = ReduceLROnPlateau(monitor="val_loss", factor=0.5, patience=3, min_lr=1e-7, verbose=1)
early_stop = EarlyStopping(monitor="val_loss", patience=8, restore_best_weights=True, verbose=1)

callbacks = [checkpoint, reduce_lr, early_stop]

from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping

# ===== Define callbacks =====
checkpoint = ModelCheckpoint(
    "best_inria_unet.h5",
    monitor="val_loss",
    verbose=1,
    save_best_only=True,
    mode="min"
)

reduce_lr = ReduceLROnPlateau(
    monitor="val_loss",
    factor=0.5,
    patience=3,
    min_lr=1e-7,
    verbose=1
)

early_stop = EarlyStopping(
    monitor="val_loss",
    patience=4,              # stop if no improvement for 8 epochs
    restore_best_weights=True,
    verbose=1
)

callbacks = [checkpoint, reduce_lr, early_stop]

# ===== Train model =====
EPOCHS = 24

history = model.fit(
    train_gen,
    steps_per_epoch=steps_per_epoch,
    validation_data=val_gen,
    validation_steps=validation_steps,
    epochs=EPOCHS,
    callbacks=callbacks,
    verbose=1
)

import matplotlib.pyplot as plt
import json

# Plot loss
plt.figure()
plt.plot(history.history.get('loss', []), label='train_loss')
plt.plot(history.history.get('val_loss', []), label='val_loss')
plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.title('Loss')
plt.savefig('train_val_loss.png', dpi=150, bbox_inches='tight')
plt.close()

# Plot IoU and Dice
plt.figure()
plt.plot(history.history.get('iou_metric', []), label='train_iou')
plt.plot(history.history.get('val_iou_metric', []), label='val_iou')
plt.xlabel('Epoch'); plt.ylabel('IoU'); plt.legend(); plt.title('IoU')
plt.savefig('iou_curve.png', dpi=150, bbox_inches='tight')
plt.close()

plt.figure()
plt.plot(history.history.get('dice_coef', []), label='train_dice')
plt.plot(history.history.get('val_dice_coef', []), label='val_dice')
plt.xlabel('Epoch'); plt.ylabel('Dice'); plt.legend(); plt.title('Dice')
plt.savefig('dice_curve.png', dpi=150, bbox_inches='tight')
plt.close()

import numpy as np
from tqdm import tqdm

# Create a fresh generator (no augmentation)
val_gen_eval = batch_generator(X_val, y_val, batch_size=BATCH, is_train=False)

all_ious, all_dices, all_accs = [], [], []
num_batches = validation_steps

for _ in tqdm(range(num_batches)):
    bx, by = next(val_gen_eval)
    preds = model.predict(bx)
    preds_bin = (preds > 0.5).astype('uint8')
    for i in range(len(preds_bin)):
        y_true = by[i].astype(bool)
        y_pred = preds_bin[i].astype(bool)
        intersection = np.logical_and(y_true, y_pred).sum()
        union = np.logical_or(y_true, y_pred).sum()
        iou = (intersection + 1e-7) / (union + 1e-7)
        dice = (2 * intersection + 1e-7) / (y_true.sum() + y_pred.sum() + 1e-7)
        acc = (preds_bin[i] == by[i]).sum() / preds_bin[i].size
        all_ious.append(iou); all_dices.append(dice); all_accs.append(acc)

metrics = {
    "mean_iou": float(np.mean(all_ious)),
    "mean_dice": float(np.mean(all_dices)),
    "mean_accuracy": float(np.mean(all_accs)),
    "num_val_patches_evaluated": len(all_ious)
}
print(json.dumps(metrics, indent=2))

# Save metrics
with open('metrics.json', 'w') as f:
    json.dump(metrics, f)

# --- IMPORTS ---
import numpy as np, json, cv2
from tqdm import tqdm
from tensorflow.keras.models import load_model
from scipy.ndimage import binary_opening, binary_closing

# --- LOAD BEST MODEL ---
model = load_model("best_inria_unet.h5", compile=False)

# --- CONFIG ---
THRESH = 0.4              # Try 0.4, 0.45, 0.5 — tune for best Dice/IoU
BATCH = 4                 # same as your eval batch
num_batches = validation_steps  # same as before

# --- GENERATOR (no augmentation) ---
val_gen_eval = batch_generator(X_val, y_val, batch_size=BATCH, is_train=False)

# --- EVALUATION ARRAYS ---
all_ious, all_dices, all_accs = [], [], []

# --- TTA + POSTPROCESSING ---
for _ in tqdm(range(num_batches)):
    bx, by = next(val_gen_eval)
    preds = model.predict(bx)

    # ----- Test-Time Augmentation -----
    # Horizontal flip TTA
    preds_flip = model.predict(bx[:, :, ::-1, :])[:, :, ::-1, :]
    preds = (preds + preds_flip) / 2.0

    # Thresholding
    preds_bin = (preds > THRESH).astype('uint8')

    # ----- Morphological cleanup -----
    for i in range(len(preds_bin)):
        mask = preds_bin[i, ..., 0]
        # Remove small noise, fill small holes
        mask = binary_opening(mask, structure=np.ones((3,3)))
        mask = binary_closing(mask, structure=np.ones((3,3)))
        preds_bin[i, ..., 0] = mask

        # --- Metrics ---
        y_true = by[i].astype(bool)
        y_pred = preds_bin[i].astype(bool)
        intersection = np.logical_and(y_true, y_pred).sum()
        union = np.logical_or(y_true, y_pred).sum()
        iou = (intersection + 1e-7) / (union + 1e-7)
        dice = (2 * intersection + 1e-7) / (y_true.sum() + y_pred.sum() + 1e-7)
        acc = (y_pred == y_true).sum() / y_true.size
        all_ious.append(iou); all_dices.append(dice); all_accs.append(acc)

# --- FINAL METRICS ---
metrics = {
    "mean_iou": float(np.mean(all_ious)),
    "mean_dice": float(np.mean(all_dices)),
    "mean_accuracy": float(np.mean(all_accs)),
    "num_val_patches_evaluated": len(all_ious),
    "threshold_used": THRESH
}

print(json.dumps(metrics, indent=2))

# --- SAVE RESULTS ---
with open('metrics_postprocessed.json', 'w') as f:
    json.dump(metrics, f)

# --- IMPORTS ---
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.models import load_model
from tqdm import tqdm
from scipy.ndimage import binary_opening, binary_closing
import json

# --- LOAD TRAINED MODEL ---
model = load_model("best_inria_unet.h5", compile=False)

# --- CONFIG ---
THRESH = 0.45  # Try 0.4–0.55 to see what works best
BATCH = 4
num_batches = 10  # show predictions from first 10 batches only for speed

# --- CREATE VALIDATION GENERATOR ---
val_gen_eval = batch_generator(X_val, y_val, batch_size=BATCH, is_train=False)

# --- ARRAYS FOR METRICS ---
all_ious, all_dices, all_accs = [], [], []

# --- LOOP THROUGH A FEW BATCHES ---
for batch_i in tqdm(range(num_batches)):
    bx, by = next(val_gen_eval)
    preds = model.predict(bx)
    preds_bin = (preds > THRESH).astype('uint8')

    # Optional cleanup: morphological filtering
    for i in range(len(preds_bin)):
        mask = preds_bin[i, ..., 0]
        mask = binary_opening(mask, structure=np.ones((3,3)))
        mask = binary_closing(mask, structure=np.ones((3,3)))
        preds_bin[i, ..., 0] = mask

        # ---- Compute metrics ----
        y_true = by[i].astype(bool)
        y_pred = preds_bin[i].astype(bool)
        intersection = np.logical_and(y_true, y_pred).sum()
        union = np.logical_or(y_true, y_pred).sum()
        iou = (intersection + 1e-7) / (union + 1e-7)
        dice = (2 * intersection + 1e-7) / (y_true.sum() + y_pred.sum() + 1e-7)
        acc = (y_pred == y_true).sum() / y_true.size
        all_ious.append(iou); all_dices.append(dice); all_accs.append(acc)

        # ---- Visualize Random Examples ----
        plt.figure(figsize=(10,3))
        plt.subplot(1,3,1)
        plt.imshow(bx[i])
        plt.title("Input Image")
        plt.axis('off')

        plt.subplot(1,3,2)
        plt.imshow(by[i].squeeze(), cmap='gray')
        plt.title("Ground Truth")
        plt.axis('off')

        plt.subplot(1,3,3)
        plt.imshow(preds_bin[i].squeeze(), cmap='gray')
        plt.title(f"Prediction\nDice={dice:.3f}, IoU={iou:.3f}")
        plt.axis('off')
        plt.show()

# --- FINAL METRICS SUMMARY ---
metrics = {
    "mean_iou": float(np.mean(all_ious)),
    "mean_dice": float(np.mean(all_dices)),
    "mean_accuracy": float(np.mean(all_accs)),
    "num_val_patches_evaluated": len(all_ious),
    "threshold_used": THRESH
}

print(json.dumps(metrics, indent=2))

# --- OPTIONAL: SAVE METRICS ---
with open('metrics_visualized.json', 'w') as f:
    json.dump(metrics, f)

print("\n✅ Visualization & evaluation completed successfully!")

# Colab: TTA ensemble (fast, no retrain)
import numpy as np, json, cv2
from tqdm import tqdm
from scipy.ndimage import binary_opening, binary_closing
from tensorflow.keras.models import load_model

model = load_model("best_inria_unet.h5", compile=False)

THRESH = 0.45
BATCH = 4
num_batches = validation_steps  # set to full validation or smaller for testing
val_gen_eval = batch_generator(X_val, y_val, batch_size=BATCH, is_train=False)

def apply_tta_predict(batch):
    # preds shape: (B, H, W, 1)
    preds_list = []
    # original
    preds_list.append(model.predict(batch))
    # hflip
    bf = batch[:, :, ::-1, :]
    p = model.predict(bf)[:, :, ::-1, :]
    preds_list.append(p)
    # vflip
    bf = batch[:, ::-1, :, :]
    p = model.predict(bf)[:, ::-1, :, :]
    preds_list.append(p)
    # rotate 90
    bf = np.rot90(batch, k=1, axes=(1,2))
    p = np.rot90(model.predict(bf), k=3, axes=(1,2))
    preds_list.append(p)
    # rotate 270
    bf = np.rot90(batch, k=3, axes=(1,2))
    p = np.rot90(model.predict(bf), k=1, axes=(1,2))
    preds_list.append(p)
    # average
    preds = np.mean(preds_list, axis=0)
    return preds

all_ious, all_dices, all_accs = [], [], []
for _ in tqdm(range(num_batches)):
    bx, by = next(val_gen_eval)
    preds = apply_tta_predict(bx)

    # optional gaussian smoothing
    for i in range(preds.shape[0]):
        preds[i,...,0] = cv2.GaussianBlur(preds[i,...,0], (3,3), 0)

    preds_bin = (preds > THRESH).astype('uint8')

    for i in range(len(preds_bin)):
        mask = preds_bin[i,...,0]
        mask = binary_opening(mask, structure=np.ones((3,3)))
        mask = binary_closing(mask, structure=np.ones((3,3)))
        preds_bin[i,...,0] = mask

        y_true = by[i].astype(bool)
        y_pred = preds_bin[i].astype(bool)
        inter = np.logical_and(y_true, y_pred).sum()
        union = np.logical_or(y_true, y_pred).sum()
        iou = (inter + 1e-7) / (union + 1e-7)
        dice = (2 * inter + 1e-7) / (y_true.sum() + y_pred.sum() + 1e-7)
        acc = (y_pred == y_true).sum() / y_true.size
        all_ious.append(iou); all_dices.append(dice); all_accs.append(acc)

metrics = {
  "mean_iou": float(np.mean(all_ious)),
  "mean_dice": float(np.mean(all_dices)),
  "mean_accuracy": float(np.mean(all_accs)),
  "num_val_patches_evaluated": len(all_ious),
  "threshold_used": THRESH
}
print(json.dumps(metrics, indent=2))

# Create a validation batch for visualization
bx, by = next(batch_generator(X_val, y_val, batch_size=8, is_train=False))
preds = model.predict(bx)
preds_bin = (preds > 0.5).astype('uint8')

import matplotlib.pyplot as plt
for i in range(min(8, len(preds_bin))):
    fig, ax = plt.subplots(1,3, figsize=(12,4))
    ax[0].imshow(bx[i]); ax[0].set_title('Image'); ax[0].axis('off')
    ax[1].imshow(by[i].squeeze(), cmap='gray'); ax[1].set_title('Ground Truth'); ax[1].axis('off')
    ax[2].imshow(preds_bin[i].squeeze(), cmap='gray'); ax[2].set_title('Prediction'); ax[2].axis('off')
    plt.savefig(f'prediction_{i}.png', dpi=150, bbox_inches='tight')
    plt.close()
print("Saved sample prediction images as prediction_0..prediction_N.png")

# Save model (best weights already saved as checkpoint)
model.save("inria_unet_final.keras")
print("Saved model to inria_unet_final.keras")

# Save model summary
with open('model_summary.txt', 'w') as fh:
    model.summary(print_fn=lambda x: fh.write(x + '\n'))

# Optionally download from Colab
from google.colab import files
files.download('inria_unet_final.keras')
files.download('model_summary.txt')
files.download('metrics.json')
files.download('train_val_loss.png')
files.download('iou_curve.png')
files.download('dice_curve.png')

import cv2
import numpy as np

def apply_morphology(mask, kernel_size=3):
    kernel = np.ones((kernel_size, kernel_size), np.uint8)
    cleaned = cv2.morphologyEx(mask.astype('uint8'), cv2.MORPH_OPEN, kernel)
    cleaned = cv2.morphologyEx(cleaned, cv2.MORPH_CLOSE, kernel)
    return cleaned

app_py = '''
"""
Simple Streamlit app to run building footprint segmentation using the trained Keras model.
Save this string to app.py (the notebook also writes app.py to disk).
"""

import streamlit as st
import numpy as np
import cv2
from tensorflow.keras.models import load_model
import tempfile
from PIL import Image

st.set_page_config(page_title="Building Footprint Segmentation", layout="centered")

st.title("Building Footprint Segmentation")
st.markdown("Upload an aerial image (RGB). The app runs the trained model and shows predicted building masks.")

# Path to model file (ensure in same directory or provide path)
MODEL_PATH = "inria_unet_final.h5"

@st.cache_resource
def load_seg_model(path):
    try:
        model = load_model(path, compile=False)
        return model
    except Exception as e:
        st.error(f"Failed to load model at {path}: {e}")
        return None

model = load_seg_model(MODEL_PATH)

uploaded_file = st.file_uploader("Upload an RGB image (jpg/png/tif)", type=['jpg', 'jpeg', 'png', 'tif', 'tiff'])
if uploaded_file is not None and model is not None:
    # Read uploaded image
    img = Image.open(uploaded_file).convert("RGB")
    img_np = np.array(img)
    h_orig, w_orig = img_np.shape[:2]

    # Resize to model input size (assumes 256x256, change if different)
    input_size = (256, 256)
    img_resized = cv2.resize(img_np, input_size, interpolation=cv2.INTER_LINEAR)
    x = img_resized.astype('float32') / 255.0
    x = np.expand_dims(x, 0)

    with st.spinner("Running inference..."):
        pred = model.predict(x)[0]
        pred_bin = (pred > 0.5).astype('uint8') * 255
        pred_up = cv2.resize(pred_bin.squeeze(), (w_orig, h_orig), interpolation=cv2.INTER_NEAREST)

    st.subheader("Input Image")
    st.image(img_np, use_column_width=True)

    st.subheader("Predicted Mask (binary)")
    st.image(pred_up, clamp=True, use_column_width=True)

    # Overlay
    overlay = img_np.copy()
    red_mask = np.zeros_like(overlay)
    red_mask[:,:,0] = pred_up
    alpha = 0.5
    overlayed = cv2.addWeighted(overlay, 1.0, red_mask, alpha, 0)

    st.subheader("Overlay")
    st.image(overlayed, use_column_width=True)

    # Download predicted mask
    import io
    from PIL import Image as PILImage
    mask_pil = PILImage.fromarray(pred_up.astype('uint8'))
    buf = io.BytesIO()
    mask_pil.save(buf, format="PNG")
    byte_im = buf.getvalue()
    st.download_button(label="Download predicted mask (PNG)", data=byte_im, file_name="pred_mask.png", mime="image/png")
else:
    if model is None:
        st.warning("Model not found. Make sure 'inria_unet_final.h5' exists in this directory.")
    st.info("Upload an image to run the model.")
'''

requirements_txt = '''
tensorflow>=2.11
tensorflow-addons
albumentations
opencv-python-headless
numpy
matplotlib
streamlit
pillow
tqdm
scikit-learn
'''

with open("app.py", "w") as f:
    f.write(app_py)

with open("requirements.txt", "w") as f:
    f.write(requirements_txt)

print("Wrote app.py and requirements.txt to disk.")

# ===== Download all key output files =====
from google.colab import files

# Model files
try:
    files.download('inria_unet_final.h5')
except:
    print("⚠️ Model file not found (check training cell).")

try:
    files.download('best_inria_unet.h5')
except:
    print("⚠️ Best model checkpoint not found.")

# Visualization & metrics
for f in ['train_val_loss.png', 'iou_curve.png', 'dice_curve.png',
          'metrics.json', 'model_summary.txt']:
    try:
        files.download(f)
    except:
        print(f"⚠️ {f} not found.")

# App and requirements
try:
    files.download('app.py')
except:
    print("⚠️ app.py not found — make sure the app creation cell ran.")

try:
    files.download('requirements.txt')
except:
    print("⚠️ requirements.txt not found — check creation cell.")